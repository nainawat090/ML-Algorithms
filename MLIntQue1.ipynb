{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HR4dEZIQXOx"
      },
      "outputs": [],
      "source": [
        "# Q1. your maching learning model perform well on training nut poorly on production data how do you debug it?\n",
        "# Q2. accurecy is high but bussiness matrix is bad ,what could be wrong?\n",
        "# Q3. how do you detect data leakage before training a model?\n",
        "# Q4. when would you prefer a simpler model over a complex one?\n",
        "# Q5. how to decide whether is feature is genuanly useful ?\n",
        "# Q6. what happens when you scale tree based models?\n",
        "# Q7. how do you handel high cardinality categorical features?\n",
        "# Q8. your moddel fit underfit what will you do?\n",
        "# Q9. hou do you identify weather a bias and varience is a problem ?\n",
        "# Q10. why does cross validation sometimes give optimistic result?\n",
        "# Q11. why is roc-auc misleading in imbalance dataset?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q1. Model works well on training but bad in production\n",
        "# Check data differences → training data and production data may be different (data drift).\n",
        "# Check preprocessing → scaling, encoding, missing values must be same in production.\n",
        "# Overfitting → model may have memorized training data, not learned patterns.\n",
        "# Feature issues → some features may be missing or calculated differently in production.\n",
        "# Test on real recent data → simulate production environment.\n",
        "\n",
        "# Q2. Accuracy high but business metric bad\n",
        "# Imbalanced data → accuracy can be misleading if one class dominates.\n",
        "# Wrong threshold → default 0.5 may not maximize business value.\n",
        "# Business cost ignored → false positives or false negatives can be expensive.\n",
        "# Use correct metrics → precision, recall, F1-score, revenue impact.\n",
        "\n",
        "# Q3. Detecting data leakage\n",
        "# Feature contains future info about target.\n",
        "# Very high validation score → too good to be true.\n",
        "# Same entity appears in train and test.\n",
        "# Features created after target occurred.\n",
        "\n",
        "# Q4. When to use simple model\n",
        "# Dataset is small → complex models overfit easily.\n",
        "# Need explanation → stakeholders must understand predictions.\n",
        "# Real-time predictions → simple models are faster.\n",
        "# Little gain from complex model → simplicity is better.\n",
        "\n",
        "# Q5. Check if a feature is useful\n",
        "# Remove the feature → see if performance drops.\n",
        "# Check feature importance (tree models, SHAP).\n",
        "# Avoid redundant features (high correlation with others).\n",
        "# Feature must make real-world sense.\n",
        "\n",
        "# Q6. Scaling tree-based models\n",
        "# Tree models (Decision Tree, Random Forest, XGBoost) don’t need scaling.\n",
        "# Splits are based on order, not magnitude.\n",
        "\n",
        "# Q7. Handle high-cardinality categorical features\n",
        "# Target encoding → replace category with mean target.\n",
        "# Frequency encoding → replace with count/frequency.\n",
        "# Group rare categories → reduce noise.\n",
        "# Use CatBoost → handles categorical features.\n",
        "# Embeddings → useful in deep learning.\n",
        "\n",
        "# Q8. Model underfitting\n",
        "# Increase model complexity (e.g., deeper tree, more neurons).\n",
        "# Add more or better features.\n",
        "# Reduce regularization.\n",
        "# Train longer / increase iterations.\n",
        "\n",
        "# Q9. Identify bias vs variance\n",
        "# Training Error\tValidation Error\tProblem\n",
        "# High\tHigh\tBias\n",
        "# Low\tHigh\tVariance\n",
        "# Low\tLow\tGood model\n",
        "# Bias → model too simple, underfits.\n",
        "# Variance → model too complex, overfits.\n",
        "\n",
        "# Q10. Cross-validation giving optimistic results\n",
        "# Data leakage between folds.\n",
        "# Random split ignores time or group.\n",
        "# Same user/entity in multiple folds.\n",
        "# Hyperparameter tuning repeatedly on CV.\n",
        "\n",
        "# Q11. ROC-AUC misleading in imbalanced data\n",
        "# ROC uses TPR vs FPR, lots of negative samples hide minority errors.\n",
        "# Model may miss rare class but still show high ROC-AUC.\n",
        "# Better metrics: Precision, Recall, F1-score, PR-AUC."
      ],
      "metadata": {
        "id": "mwwukWoEvg7m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}